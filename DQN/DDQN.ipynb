{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q神经网络\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class state_to_action_mlp(nn.Module):\n",
    "#     def __init__(self, n_starts,n_actions,hidden_dim = 128): #n_starts与n_actions就是输入与输出得维数\n",
    "#         super(state_to_action_mlp,self).__init__();\n",
    "#         self.fc1 = nn.Linear(n_starts,hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim,n_actions)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class state_to_action_mlp(nn.Module):\n",
    "    def __init__(self, n_states, n_actions,hidden_dim=128):\n",
    "        super(state_to_action_mlp, self).__init__()\n",
    "        \n",
    "        # hidden layer\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #  advantage\n",
    "        self.advantage_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "        \n",
    "        # value\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.hidden_layer(state)\n",
    "        advantage = self.advantage_layer(x)\n",
    "        value     = self.value_layer(x)\n",
    "        return value + advantage - advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e914b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义经验回放\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class replybuffer(object):#经验回放就是一个队列，有对策功能就是输入数据，采样数据\n",
    "    #作为一个队列其就需要容量和存储的区域，这些我们给了以后就是编写其类内部实现这些函数的功能了\n",
    "    def __init__(self,capacity:int):\n",
    "        self.capacity = capacity #定义经验回放的容量\n",
    "        self.buffle = deque(maxlen=self.capacity)\n",
    "    def push(self,transitions):\n",
    "        self.buffle.append(transitions)\n",
    "    def clear(self):\n",
    "        self.buffle.clear()\n",
    "    def sample(self,batch_size,sequntial):\n",
    "        if batch_size > len(self.buffle):\n",
    "            batch_size = len(self.buffle)\n",
    "        if sequntial:\n",
    "            rand = random.randint(0,len(self.buffle)-batch_size)#从以前的旧数据中进行采样而新进入的数据采样\n",
    "            batch = [self.buffle[i] for i in range(rand,rand+batch_size)] #从中选取batch_size的数据采样作为数据训练\n",
    "        else:\n",
    "            batch = random.sample(self.buffle,batch_size)\n",
    "        return zip(*batch)#把储存数据的地址给传输回去\n",
    "    def __len__(self):\n",
    "        return len(self.buffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "class cnfig(object):\n",
    "    def __init__(self):\n",
    "        self.n_states = None  #状态的维度和动作在离散空间的情况下我们是可以自己设定好的，就是固定了输入和我们的输出\n",
    "        self.n_actions = None  #续上：连续的话需要重新考虑一下\n",
    "        self.algo_name = 'DoubleDQN' # 算法名称\n",
    "        self.env_name = 'grid_env' # 环境名称\n",
    "        self.seed = 1 # 随机种子\n",
    "        self.train_eps = 100 # 训练回合数\n",
    "        self.test_eps = 10  # 测试回合数\n",
    "        self.max_steps = 200 # 每回合最大步数\n",
    "        self.gamma = 0.99 # 折扣因子\n",
    "        self.lr = 0.001 # 学习率\n",
    "        self.epsilon_start = 0.95 # epsilon初始值\n",
    "        self.epsilon_end = 0.01 # epsilon最终值\n",
    "        self.epsilon_decay = 1000 # epsilon衰减率\n",
    "        self.buffer_size = 50000 # ReplayBuffer容量\n",
    "        self.batch_size = 128 # ReplayBuffer中批次大小\n",
    "        self.target_update = 500 # 目标网络更新频率\n",
    "        self.hidden_dim = 256 # 神经网络隐藏层维度\n",
    "        self.use_trained_model = False\n",
    "        if torch.cuda.is_available(): # 是否使用GPUs\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "def smooth(data, weight=0.9):  \n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "\n",
    "def print_cfgs(cfg):\n",
    "    cfg_dict = vars(cfg)\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in cfg_dict.items():\n",
    "        if v.__class__.__name__ == 'list':\n",
    "            v = str(v)\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55096d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import true\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.serialization import MAP_LOCATION\n",
    "\n",
    "#作为一个强化学习算法其实就是三个主要的函数就是采样/预测/更新，任何强化学习算法都是这三个步骤\n",
    "class DDQN(object):\n",
    "    #作为一个强化学习算法其应该拥有状态,奖励，动作，折扣因子，学习率，探索率，是否更新标志\n",
    "    \n",
    "    #一般还需要Q表，但是现在是DDQN是神经网络所以我们需要目标网络和策略网络，神经网络就要有优化器，然后我们采样要从经验回访中得到\n",
    "    \n",
    "    ##其实就是可以分为#与设备相关#与奖励相关#与探索策略相关#与网络相关   4大类！！！！\n",
    "    def __init__(self,cfg:cnfig):\n",
    "        #与设备相关\n",
    "        self.device = torch.device(cfg.device)\n",
    "        #与奖励相关\n",
    "        self.gamma = cfg.gamma\n",
    "        #与探索策略相关\n",
    "        self.epslion = cfg.epsilon_start\n",
    "        self.sample_count = 0\n",
    "        self.epslion_start = cfg.epsilon_start\n",
    "        self.epslion_end = cfg.epsilon_end\n",
    "        self.epslion_decay = cfg.epsilon_decay\n",
    "        #与网络相关\n",
    "        self.lr = cfg.lr\n",
    "        self.state = cfg.n_states\n",
    "        self.action = cfg.n_actions\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.target_net = state_to_action_mlp(cfg.n_states,cfg.n_actions,hidden_dim=cfg.hidden_dim).to(cfg.device) #这一步直接加载模型到设备上嘛？我不是很理解欸看看是否可以优化\n",
    "        self.target_update = cfg.target_update\n",
    "        self.policy_net = state_to_action_mlp(cfg.n_states,cfg.n_actions,hidden_dim=cfg.hidden_dim).to(cfg.device)\n",
    "        if cfg.use_trained_model:\n",
    "            self.policy_net.load_state_dict(torch.load(\"grid_ddqn.pt\"),MAP_LOCATION = cfg.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())#把策略网络的参数加载进目标网络\n",
    "        self.optim = optim.Adam(self.policy_net.parameters(),lr = cfg.lr)\n",
    "        self.memory = replybuffer(cfg.buffer_size)#经验回访池的大小难道不应该大于batch_size的值从而保证可以接收到更多的数据嘛？\n",
    "        \n",
    "        self.update_flag = False\n",
    "    def sample(self,state):\n",
    "        self.sample_count += 1\n",
    "        self.epslion = self.epslion_end + (self.epslion_start - self.epslion_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epslion_decay)  \n",
    "        \n",
    "        if random.random() > self.epslion:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0) #模型1已经加载到了cuda上去了，但是我们采样的数据还没有，所以我们要把数据页加载上去\n",
    "                q_values = self.policy_net(state) #输出的q_values的真实的形状是（batch_size,n_actions）batch_size,n_actions都是具体的数\n",
    "                action = q_values.max(1)[1].item() #这个就是把动作的序号给取出来\n",
    "        else:\n",
    "            action = random.randrange(self.action)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def perdict(self,state):\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        pre_q_values = self.policy_net(state)\n",
    "        pre_action = pre_q_values.max(1)[1].item()\n",
    "        return pre_action\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size: #要经验池里面有足够的训练样本我们才能够开始训练\n",
    "            return \n",
    "        else:\n",
    "            if not self.update_flag:#这一步就是告诉我们什么时候策略网络开始更新了\n",
    "                print(\"开始更新策略！\")\n",
    "                self.update_flag = True\n",
    "        #训练是从经验回放中取样本\n",
    "        state_batch,action_batch,reward_batch,next_state_batch,done_batch = self.memory.sample(self.batch_size,True)\n",
    "        #把我们的数据都转为tensor的格式输入给神经网络，但是要记得加载到cuda上去\n",
    "        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1) #action本来是【b】一个列表得格式得现在把他变成【b,1】这样得一个格式\n",
    "        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "        done_batch = torch.tensor(np.array(done_batch), device=self.device,dtype=torch.float32).unsqueeze(1)\n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float32)\n",
    "        #我们的这个Q函数是针对离散动作得Q函数，详细得针对连续动作的Q函数要去easy_RL里面学习\n",
    "        #DQN 的损失只训练每个样本“实际执行的动作”的 Q 值，所以我们要去找到我们对应执行的动作\n",
    "        q_value_batch = self.policy_net(state_batch).gather(dim=1,index = action_batch)#实际的q值 #利用gather函数我能够直接找到我样本对应的action的q_value,为什么我们要找到这些值\n",
    "        with torch.no_grad():\n",
    "            next_max_q_value_batch = self.target_net(next_state_batch).max(1)[0].detach().unsqueeze(1)\n",
    "                #torch.max是返回两个东西一个是最大值的列表一个是最大值在每行的位置的索引，返回的第二个就类似于action列表的作用\n",
    "                #torch.max(next_q_value_batch,dim=1)[1].unsqueeze(1)返回的就是每行最大动作值的一个列表，为我们的目标动作即max（q，a）\n",
    "                #下一步就是计算损失了损失函数就是E（（r+ymax q（n_s，a）- q(s,a)）平方）\n",
    "            expected_q_vlaue_batch = reward_batch + self.gamma * next_max_q_value_batch * (1-done_batch) #就是看看done_batch是不是中止如果是中止的话那么我们就不需要后面的状态了\n",
    "        # next_target_value_batch = self.target_net(next_state_batch) #下一个状态对应的目标网络Q值\n",
    "        # next_target_value_batch = next_target_value_batch.gather(1,torch.max(next_q_value_batch,dim=1)[1].unsqueeze(1))\n",
    "        # #torch.max是返回两个东西一个是最大值的列表一个是最大值在每行的位置的索引，返回的第二个就类似于action列表的作用\n",
    "        # #torch.max(next_q_value_batch,dim=1)[1].unsqueeze(1)返回的就是每行最大动作值的一个列表，为我们的目标动作即max（q，a）\n",
    "        # #下一步就是计算损失了损失函数就是E（（r+ymax q（n_s，a）- q(s,a)）平方）\n",
    "        # expected_q_vlaue_batch = reward_batch + self.gamma * next_target_value_batch * (1-done_batch) #就是看看done_batch是不是中止如果是中止的话那么我们就不需要后面的状态了\n",
    "        loss_function = nn.MSELoss()\n",
    "        loss = loss_function(q_value_batch,expected_q_vlaue_batch)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        #这里需要小心因为可能会导致梯度爆炸，所以我们在反向传播之后要对我们的梯度进行裁剪\n",
    "        #这里还需要学习，我们现在按照番薯的裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10.0)\n",
    "        self.optim.step() #反向传播之后只是计算出来了梯度值，我们实际上进行运算的话还是需要调用step来进行更新\n",
    "        #但是我们不会删除梯度，所以其实在每次loss_backward之前我们要情况一次梯度self.optim.zero_grad()\n",
    "        if self.sample_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "556cc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前面我们已经把强化学习算法的框架搭好了，算法内的配置和采样，预测，更新都写好了\n",
    "#所以我们现在要搭训练的环境！\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from env_design.envpy import grid_env\n",
    "\n",
    "#构建一个万能的随机函数\n",
    "def all_seed(seed = 1):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def env_agent_config(cfg:cnfig):\n",
    "    env = grid_env()\n",
    "    all_seed(cfg.seed)\n",
    "    #这个地方针对不同的函数我们也是要改动的\n",
    "    n_states = env.state_space() #???难道说这个函数可以直接观测出来我们需要的状态数字吗？\n",
    "    n_actions = env.action_space()\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    cfg.n_states = n_states\n",
    "    cfg.n_actions = n_actions\n",
    "    agent = DDQN(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b917691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(cfg:cnfig,env,agent:DDQN):\n",
    "    print(\"开始训练\")\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        #单回合的奖励和但会和的训练长度\n",
    "        ep_rewards = 0;\n",
    "        ep_steps = 0;\n",
    "        state,info= env.reset()\n",
    "        #接下来我就是要进行每个回合的训练了\n",
    "        for ep_steps in range(cfg.max_steps):\n",
    "            action = agent.sample(state)\n",
    "            next_state,reward,terminated,truncated,_ = env.step(action) #我们采样到动作之后将动作哦返回到环境中进行更新\n",
    "            done = terminated or truncated\n",
    "            agent.memory.push((state,action,reward,next_state,done))\n",
    "            state = next_state\n",
    "            #我们在采样的过程中会不断去检查我们的经验回放的值，如果经验回放池里面的数据足够的话我们就会走一步就去迭代我们的模型一次\n",
    "            agent.update()\n",
    "            ep_rewards += reward\n",
    "            ep_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        steps.append(ep_steps)\n",
    "        rewards.append(ep_rewards)\n",
    "        if i_ep%10 ==0 :\n",
    "            print(f\"回合数 {i_ep} / {cfg.train_eps} : rewards:{max(rewards)} epsilion:{agent.epslion}\")\n",
    "    print(\"完成训练！\")\n",
    "    return {\"rewards\" : rewards} #输出的竟然是一个字典？匪夷所思\n",
    "\n",
    "def test(cfg:cnfig,env,agent:DDQN):\n",
    "    print(\"开始测试！\")\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for i__eps in range(cfg.test_eps):\n",
    "        ep_rewards = 0\n",
    "        ep_steps = 0\n",
    "        state,info= env.reset()\n",
    "        for ep_steps in range(cfg.max_steps):\n",
    "            action = agent.sample(state)\n",
    "            next_state,reward,terminated,truncated,_ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            ep_rewards += reward\n",
    "            ep_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_rewards)\n",
    "        print(f\"回合数 {i__eps} / {cfg.test_eps} : rewards:{ep_rewards} , 结束回合数 : {ep_steps}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {'rewards':rewards}\n",
    "\n",
    "\n",
    "# from IPython.display import display, clear_output\n",
    "# import time\n",
    "\n",
    "# def test(cfg: cnfig, env, agent: DDQN, render=True, delay=0.05):\n",
    "#     print(\"开始测试！\")\n",
    "#     rewards = []\n",
    "#     steps = []\n",
    "#     ax = None  # 复用画布，避免不停创建新 figure\n",
    "#     handle = None\n",
    "#     for i_eps in range(cfg.test_eps):\n",
    "#         ep_rewards = 0\n",
    "#         state, info = env.reset()\n",
    "\n",
    "#         for step_idx in range(cfg.max_steps):\n",
    "#             if render:\n",
    "#                 ax = env.render(ax=ax)\n",
    "#                 if handle is None:\n",
    "#                     handle = display(ax.figure, display_id=True)\n",
    "#                 else:\n",
    "#                     handle.update(ax.figure)\n",
    "#                 plt.pause(delay)\n",
    "\n",
    "#             action = agent.sample(state)\n",
    "#             next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "#             state = next_state\n",
    "#             ep_rewards += reward\n",
    "\n",
    "#             if terminated or truncated:\n",
    "#                 # 终止状态也渲染一帧\n",
    "#                 if render:\n",
    "#                     ax = env.render(ax=ax)\n",
    "#                     if handle is None:\n",
    "#                         handle = display(ax.figure, display_id=True)\n",
    "#                     else:\n",
    "#                         handle.update(ax.figure)\n",
    "#                     plt.pause(delay)\n",
    "#                 break\n",
    "\n",
    "#         rewards.append(ep_rewards)\n",
    "#         print(f\"回合数 {i_eps} / {cfg.test_eps} : rewards:{ep_rewards}\")\n",
    "#     print(\"完成测试！\")\n",
    "#     return {'rewards': rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f7ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：2，动作空间维度：4\n",
      "开始训练\n",
      "开始更新策略！\n",
      "回合数 0 / 100 : rewards:-264 epsilion:0.9462475099833519\n",
      "回合数 10 / 100 : rewards:-212 epsilion:0.9095367200246638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39muse_trained_model:\n\u001b[0;32m      3\u001b[0m     env,agent \u001b[38;5;241m=\u001b[39m env_agent_config(cfg)\n\u001b[1;32m----> 4\u001b[0m     res_dic \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     plot_rewards(res_dic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m], title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining curve on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39malgo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 测试\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(cfg, env, agent)\u001b[0m\n\u001b[0;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#我们在采样的过程中会不断去检查我们的经验回放的值，如果经验回放池里面的数据足够的话我们就会走一步就去迭代我们的模型一次\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m ep_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     20\u001b[0m ep_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[18], line 96\u001b[0m, in \u001b[0;36mDDQN.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(q_value_batch,expected_q_vlaue_batch)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m#这里需要小心因为可能会导致梯度爆炸，所以我们在反向传播之后要对我们的梯度进行裁剪\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#这里还需要学习，我们现在按照番薯的裁剪\u001b[39;00m\n\u001b[0;32m     99\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)\n",
      "File \u001b[1;32md:\\Anconda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = cnfig()\n",
    "if not cfg.use_trained_model:\n",
    "    env,agent = env_agent_config(cfg)\n",
    "    res_dic = train(cfg,env,agent)\n",
    "    plot_rewards(res_dic['rewards'], title=f\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  \n",
    "    # 测试\n",
    "    res_dic = test(cfg, env, agent)\n",
    "    plot_rewards(res_dic['rewards'], title=f\"testing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  # 画出结果\n",
    "else:\n",
    "    env,agent = env_agent_config(cfg)\n",
    "    res_dic = test(cfg, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e09a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把我的模型保存下来\n",
    "torch.save(agent.policy_net.state_dict(), \"grid_ddqn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
