{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad865d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment passes all checks!\n"
     ]
    }
   ],
   "source": [
    "#设计一个环境，环境是一个类，所以说我们要继承gymnasium.env这个类，他包含了所有的环境需要结构\n",
    "#一个环境最基础的是需要定义一个观测空间和一个动作空间\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch._higher_order_ops.triton_kernel_wrap import triton_kernel_wrapper_functional_fake_tensor_mode\n",
    "from torch.backends.cuda import flash_sdp_enabled\n",
    "\n",
    "###################################################################\n",
    "#定义了网格大小\n",
    "#定义了初始位置和目标位置\n",
    "#定义了观测空间\n",
    "#定义了动作空间和动作的含义\n",
    "##################################################################\n",
    "class grid_env(gym.Env): #继承这个类有鸡毛用錒\n",
    "    #size_cow size_rol 代表网格世界的大小，我们需要的是一个自定义长方形的\n",
    "    def __init__(self,size_cow,size_rol):\n",
    "        super().__init__()\n",
    "        #定义环境的大小\n",
    "        self.size_cow = size_cow\n",
    "        self.size_rol = size_rol\n",
    "        #定义了环境的大小后我们要定义我们智能体初始在的位置\n",
    "        self._action_location = np.array([-1,-1],dtype=np.int32)\n",
    "        self._target_location = np.array([-1,-1],dtype=np.int32)\n",
    "\n",
    "        #我们已经把智能体的初始位置在环境中定好了，也就是智能体已经放到环境中去了\n",
    "        #还有就是我们作为程序员通过代码知道的目前智能体的在空间的状态\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"agent\":gym.spaces.Box(low=np.array([0,0]),\n",
    "            high=np.array([self.size_cow-1,self.size_rol-1]),\n",
    "            dtype=int),\n",
    "\n",
    "            \"target\":gym.spaces.Box(low=np.array([0,0]),\n",
    "            high=np.array([self.size_cow-1,self.size_rol-1]),\n",
    "            dtype=int),\n",
    "        })#意味着gym.state还有其他的定义方式，我们可以去space这个类里面去找\n",
    "        \n",
    "        #所以我们要定义我们智能体可以采取的动作\n",
    "        self.action_space = gym.spaces.Discrete(4) #DIScrete这个函数啥意思呀？\n",
    "        #定义我们具体的四个动作可以干什么\n",
    "        self._action_to_direction = {\n",
    "            #其实就是我如果采取了这个行为以后，得到的东西，这个得到的东西是为了智能体的移动\n",
    "            0:np.array([1,0]), #向右\n",
    "            1:np.array([0,1]), #向上\n",
    "            2:np.array([-1,0]), #向左\n",
    "            3:np.array([0,-1]), #向下\n",
    "            #难道说这个数组列表是从下往上数行的吗？感觉这个向上和向下完全不能理解\n",
    "        }\n",
    "    #########################################这段代码的作用不好理解####################################################\n",
    "    #写一个_get_obs这个辅助的函数来帮助我们将环境内部的状态改变成我们程序员观测的格式\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\":self._agent_location,\n",
    "            \"target\":self._target_location\n",
    "        }\n",
    "    #在调用env.step的时候会返回info，所以这个info我们可以自己去设定，需要返回什么info我们自己去设置就可以了\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\" : np.linalg.norm(self._agent_location - self._target_location,ord=1)\n",
    "        }\n",
    "    #################################################################################################################\n",
    "\n",
    "    #reset() 方法启动一个新的回合,在我们的网格世界环境中，reset() 随机地将智能体和目标放置在网格上，确保它们不会在同一个位置开始。\n",
    "    def reset(self,seed : Optional[int] = None,options : Optional[dict] = None):\n",
    "        \"\"\"Start a new episode.\n",
    "\n",
    "        Args:\n",
    "            seed: Random seed for reproducible episodes\n",
    "            options: Additional configuration (unused in this example)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (observ ation, info) for the initial state\n",
    "        \"\"\"\n",
    "        # IMPORTANT: Must call this first to seed the random number generator\n",
    "        super().reset(seed = seed)\n",
    "        #随意的放置这个agent到任何一个网格的位置上，实际上这个位置是我们可以自定义的，可以放在OPtional里面来传入\n",
    "        self._agent_location = self.np_random.integers(low=0,high=[self.size_cow,self.size_rol],size=2,dtype = int)\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._agent_location,self._target_location):\n",
    "\n",
    "            self._target_location = self.np_random.integers(low=0,high=np.array([self.size_cow,self.size_rol]),size=2,dtype = int)\n",
    "\n",
    "        # #固定一个出发地和一个目标地\n",
    "        # self._agent_location = np.array([2,0])\n",
    "        # self._target_location = np.array([2,9])\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation,info\n",
    "        #step() 方法包含核心环境逻辑。它接收一个动作，更新环境状态，并返回结果。物理、游戏规则和奖励逻辑都在这里。\n",
    "\n",
    "        #对于网格世界，我们需要：\n",
    "        # 1. 将离散动作转换为移动方向 \n",
    "        # 2. 更新智能体的位置（带边界检查）\n",
    "        # 3. 根据是否达到目标计算奖励 \n",
    "        # 4. 判断回合是否应该结束 \n",
    "        # 5. 返回所有所需信息\n",
    "\n",
    "    def step(self,action):\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        self._agent_location=np.clip(self._agent_location + direction,np.array([0,0]),np.array([self.size_cow-1,self.size_rol-1]))\n",
    "\n",
    "        terminated = np.array_equal(self._agent_location,self._target_location)\n",
    "\n",
    "        truncated = False\n",
    "        distance = np.linalg.norm(self._agent_location - self._target_location)\n",
    "        reward = 1 if terminated else -1*distance\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# This will catch many common issues\n",
    "try:\n",
    "    env = grid_env(5,10)\n",
    "    check_env(env)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
